{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4e92db-0969-4201-885c-69d9ea76bb96",
   "metadata": {},
   "source": [
    "# HW 7 ST 590 Solution\n",
    "Courtesy of Mark Austin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6669e31-b9a0-44dd-bb5d-6381fd5a059b",
   "metadata": {},
   "source": [
    "## Read in and Combine Data\n",
    "Read in the winequality-red.csv and winequality-white.csv files available on the uci machine\n",
    "learning repository site.\n",
    "\n",
    "I use `pandas` function `read_csv()` to read in both the red and white wine quality data sets.  The `sep=` option was needed because both files are `;` delimited.  \n",
    "\n",
    "I store the red wine data in dataframe `wine_red` and use the `.head()` method to confirm data read as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed173676-0734-4a2d-b8a9-983516283e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wine_red = pd.read_csv(\"winequality-red.csv\", sep = \";\")\n",
    "wine_red.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7477c99-e77b-4486-bde9-6aa9709dc8d2",
   "metadata": {},
   "source": [
    "I store the white wine data in dataframe `wine_white` and use the `.tail()` method to confirm data read as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a700d36d-be1a-4bd9-b498-88714c18c80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_white = pd.read_csv(\"winequality-white.csv\", sep = \";\")\n",
    "wine_white.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d96f6-76b8-4bc5-926a-9363fdcfa6da",
   "metadata": {},
   "source": [
    "Combine these two datasets and create a new variable that represents the type of wine (red or white).  \n",
    "\n",
    "I decided to go ahead and create the new `type` column earlier by setting white to equal 1 and red to equal 0.  I used integers because we'll need integers for `type` later in logistic regression.   \n",
    "\n",
    "I use the `pd.concat()` function to combine the red and white dataframes into a new `wine_full` dataframe using the `ignore_index` option so the index numbers will reset.  I look at the beginning and end of `wine_full` and confirm those parts are like earlier data from the seperate dataframes thus data combined as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae195f6-0a2f-487e-ac76-4610ad144db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6492            6.2              0.21         0.29             1.6      0.039   \n",
       "6493            6.6              0.32         0.36             8.0      0.047   \n",
       "6494            6.5              0.24         0.19             1.2      0.041   \n",
       "6495            5.5              0.29         0.30             1.1      0.022   \n",
       "6496            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "6492                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "6493                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "6494                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "6495                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  type  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     0  \n",
       "2         9.8        5     0  \n",
       "3         9.8        6     0  \n",
       "4         9.4        5     0  \n",
       "...       ...      ...   ...  \n",
       "6492     11.2        6     1  \n",
       "6493      9.6        5     1  \n",
       "6494      9.4        6     1  \n",
       "6495     12.8        7     1  \n",
       "6496     11.8        6     1  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_red[\"type\"] = 0\n",
    "wine_white[\"type\"] = 1\n",
    "wine_full = pd.concat([wine_red, wine_white], ignore_index = True)\n",
    "wine_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759296c-64bd-4e4b-b9be-e406e4b4f679",
   "metadata": {},
   "source": [
    "## Split the Data\n",
    "Split up the data set into a training and test set. For this, I want you to use stratified sampling to\n",
    "make sure that you have a similar proportion of white and red wines in the training and test sets. This\n",
    "can be done with the `train_test_split()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9a018-87b4-449a-b9c9-1967190fdd18",
   "metadata": {},
   "source": [
    "The `train_test_split()` function from `sklearn.model_selection` is used to split the data into training and test sets.  For the X parts I use the `.drop()` method to select all columns except for `alcohol` and `type` which I later use as inputs for the Y parts.  I set `test_size` of .20 or an 80/20 train/test split ratio.  I add the `stratify` option set to `type` to get similar proportions based on type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb443df-b70b-4279-b768-b5990630389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " wine_full.drop(columns = [\"alcohol\", \"type\"]),\n",
    "    wine_full[[\"alcohol\", \"type\"]], \n",
    " test_size = 0.20,\n",
    " stratify = wine_full[\"type\"],\n",
    " random_state = 29)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993bda3-b45a-4a3d-9074-77d6eb0143bf",
   "metadata": {},
   "source": [
    "I start this part by using the `.value_counts()` method to see the proportion of wine types in the full data so that I can compare to make sure I correctly did stratification by type using the y data with `type`.  Then I look at the new `y_train` and `y_test` counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5354c822-cf19-4f34-8cd0-f7987ebbc8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.754\n",
       "0    0.246\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(wine_full[\"type\"].value_counts()/len(wine_full[\"type\"]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2d51aa-278c-41f6-af0a-a8726d45d62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.754\n",
       "0    0.246\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(y_train[\"type\"].value_counts()/len(y_train[\"type\"]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c865bd-89f1-48a2-a93c-8609aa014354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.754\n",
       "0    0.246\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(y_test[\"type\"].value_counts()/len(y_test[\"type\"]), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6b1c6-0b04-477b-9e02-6ac9a950729c",
   "metadata": {},
   "source": [
    "The new wine proportions are the same as expected after using stratified sampling confirming this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84215a81-1a8e-4520-917a-fcc97c99f151",
   "metadata": {},
   "source": [
    "### Standardize Predictors\n",
    "This part was optional but I decided to do this for practice at this point using code derived from lecture examples.\n",
    "\n",
    "The key part here is that the TRAIN mean and standard deviations are used to standardize BOTH the train and test predictor sets.  Within that framework, `np.mean` `np.std` and the `.apply()` method are used to carry out the standardization calculations for all the predictor columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b79948-5107-4a3c-b810-8c4f48f1a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "means = X_train.apply(np.mean, axis = 0)\n",
    "\n",
    "stds = X_train.apply(np.std, axis = 0)\n",
    "\n",
    "X_train = X_train.apply(lambda x: (x-np.mean(x))/np.std(x), axis = 0)\n",
    "\n",
    "X_test = X_test.apply(lambda x: (x-np.mean(x))/np.std(x), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167487d8-caf4-4be9-b26c-29da6223daa8",
   "metadata": {},
   "source": [
    "## Regression Task (alcohol as Response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac386459-814a-4505-b04d-955e5b3d44fb",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8c7c8-2a97-4a30-a965-3a68d08b9846",
   "metadata": {},
   "source": [
    "#### Fit four different multiple linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7136369-2388-428b-9dfa-da2a62acca65",
   "metadata": {},
   "source": [
    "For all four MLR models I use `cross_validate` function from `sklearn.model.selection` to do 5 fold CV with `neg_mean_squared_error` as the scoring criteria.  Each model uses `LinearRegression()` from `sklearn.linear_model` with the same y response of `alchohol` from `y_train`.  \n",
    "\n",
    "The first model tested in `cv_few_reg` is one with very few predictors.  In this case I'm only using `[\"chlorides\", \"pH\"]` from the training dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34da7807-434c-4650-bea0-15fd18e4294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "cv_few_reg = cross_validate(\n",
    "    LinearRegression(),\n",
    "    X_train[[\"chlorides\", \"pH\"]],\n",
    "    y_train[\"alcohol\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282e21a-e675-4f17-9778-d2815d0f9a8b",
   "metadata": {},
   "source": [
    "The second model tested in `cv_many_reg` is one with many predictors to be very different from the first model.  In this case I'm using `[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]` from the training dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c0e2164-e023-46c0-9208-ed163391d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_many_reg = cross_validate(\n",
    "    LinearRegression(),\n",
    "    X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "    y_train[\"alcohol\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93886489-48ba-40ac-b884-804e38d89d5f",
   "metadata": {},
   "source": [
    "At least one should include interaction terms.  The third model tested in `cv_inter_reg` add interaction to the variables used in the first few predictor model.  `PolynomialFeatures(interaction_only=True, include_bias = False)` is used to add an interaction column to the first few predictor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc3ead01-c587-4fa5-80f3-c7e611b87e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "inter_poly = PolynomialFeatures(interaction_only=True, include_bias = False)\n",
    "inter_train = inter_poly.fit_transform( X_train[[\"chlorides\", \"pH\"]])\n",
    "\n",
    "cv_inter_reg = cross_validate(\n",
    "    LinearRegression(),\n",
    "    inter_train,\n",
    "    y_train[\"alcohol\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9f78f-5d0e-451f-90fb-d442e98dbefe",
   "metadata": {},
   "source": [
    "At least one should include some polynomial terms (you may want to standardize your predictors\n",
    "but that is up to you)\n",
    "\n",
    "For the fourth model tested in `cv_poly_reg` I add qudratic and cubic terms using `PolynomialFeatures(degree = (2,3), include_bias = False)` with the same columns from the first few predictor model.  Note that this also adds interaction between all these terms too so in the end we've got the first, second, and third degree terms and their two way interaction terms as predictor in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f446a093-4909-4db4-9f32-9d72dd37c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = (2,3), include_bias = False)\n",
    "poly_train = poly.fit_transform( X_train[[\"chlorides\", \"pH\"]])\n",
    "\n",
    "cv_poly_reg = cross_validate(\n",
    "    LinearRegression(),\n",
    "    poly_train,\n",
    "    y_train[\"alcohol\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b0cb3-9fec-4479-b43d-38c6dd4eade2",
   "metadata": {},
   "source": [
    "Use CV to select your best MLR model.\n",
    "\n",
    "There was no set criteria stated for this so as stated earlier I used MSE.  To get the best model, I summed the test scores for each of the four models to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ce3bb5-3271-4035-af59-083be617bf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5587561158797256 1.6570650008179095 2.550658449307791 2.63773341464033\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(-sum(cv_few_reg['test_score'])), \n",
    "      np.sqrt(-sum(cv_many_reg['test_score'])), \n",
    "      np.sqrt(-sum(cv_inter_reg['test_score'])),\n",
    "     np.sqrt(-sum(cv_poly_reg['test_score']))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a67f6d4-0cf3-43e5-9b9e-23ce85c32e0b",
   "metadata": {},
   "source": [
    "Lowest summed MSE went with `cv_many_reg` thus it is the best of these four MLR models.  Next refit this model using the full training set again using `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e2c786-ac75-482d-8b4a-89c66ed85a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "mlr_many = linear_model.LinearRegression()\n",
    "mlr_many_fit = mlr_many.fit\\\n",
    "            (X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\\\n",
    "             y_train[\"alcohol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff876e-a36d-4eca-a076-5cf68673bd9d",
   "metadata": {},
   "source": [
    "#### Fit a LASSO model with a set of predictors of your choosing\n",
    "Use at least five predictors  \n",
    "Use CV to select the tuning parameter  \n",
    "\n",
    "For all the other models I used the same predictors as in the best MLR model namely `[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]`.  This allows me to simplify some code later in this assignment.  \n",
    "\n",
    "For the LASSO model, I started with `LassoCV()` from `sklearn.linear_model` to get the best alpha parameter using 5 fold CV.  This best alpha is stored as part of `lasso_alpha_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbbdbee2-23a6-4c8f-a8ed-59a227ce1916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008299016722122313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "lasso_alpha_model = LassoCV(cv=5, random_state=10) \\\n",
    "    .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"alcohol\"])\n",
    "\n",
    "print(lasso_alpha_model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858207e-2c62-43ee-a6d9-f5abc80e9c5c",
   "metadata": {},
   "source": [
    "Given the really low alpha value, I can already tell the full model from earlier is likely favored.  I use this alpha value in the `Lasso()` function with the full training data to fit the LASSO model `lasso_model_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1af5fd0-3f5a-491c-9712-2f65858720c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_fit = Lasso(lasso_alpha_model.alpha_)\\\n",
    "    .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"alcohol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa33f8-8abe-4031-bfd6-6c8f58d6d18d",
   "metadata": {},
   "source": [
    "#### Fit a Ridge Regression model with a set of predictors of your choosing\n",
    "Use at least five predictors  \n",
    "Use CV to select the tuning parameter  \n",
    "\n",
    "Again, this model uses the same predictors but this time I am doing Ridge Regression first using `RidgeCV()` to get the best alpha paramter.  I found in the doc that the default for alphas was only `(0.1, 1.0, 10.0)` so I added a few more values to see what would happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2748da8a-736b-438c-aa79-a76ec3a1b75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "\n",
    "\n",
    "ridge_alpha_model = RidgeCV(cv=5, alphas = (0.05, 0.1, 0.2, 0.5, 1, 3, 10)) \\\n",
    "    .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"alcohol\"])\n",
    "\n",
    "print(ridge_alpha_model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b27dfdc-e2ff-4457-acb9-20274254ff96",
   "metadata": {},
   "source": [
    "The best parameter ended up being the `3` that I added to the default values.  I next use that default value in `Ridge()` to fit the model using the full training data and save as `ridge_model_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f9f4f7-590c-4af1-907d-86da950088e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model_fit = Ridge(ridge_alpha_model.alpha_)\\\n",
    "    .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"alcohol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035315b-2899-4605-a3b9-fbb53264467a",
   "metadata": {},
   "source": [
    "#### Fit an Elastic Net model with a set of predictors of your choosing\n",
    "Use at least five predictors  \n",
    "Use CV to select the tuning parameter  \n",
    "\n",
    "Again, this model uses the same predictors but this time I am doing Elastic Net Regression using `ElasticNetCV()` to get the best alpha and L1 paramters.  In this case I used the same tuning values given in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d64789-4397-4a48-a6da-d897ab43b29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008468384410328895\n",
      "0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "\n",
    "net_tune_model = ElasticNetCV(cv=5,\n",
    "                              n_alphas = 300,\n",
    "                              l1_ratio = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.98, 0.99, 1],\n",
    "                             random_state = 30\n",
    "                             ) \\\n",
    "    .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"alcohol\"])\n",
    "\n",
    "print(net_tune_model.alpha_)\n",
    "print(net_tune_model.l1_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d8dee-d5cd-45ad-b6d1-fff7c272a018",
   "metadata": {},
   "source": [
    "Finally I use these parameter values with `ElasticNet()` to fit the model using the full training data and store this as `net_model_fit`.  It is important to note at this point that `0.83` was chosen by CV as the L1 Ratio which is saying this is very close to the LASSO model from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11b0a5aa-535f-475f-9ec5-06473ae84a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_model_fit = ElasticNet(alpha = net_tune_model.alpha_,\n",
    "                      l1_ratio = net_tune_model.l1_ratio_)\\\n",
    "    .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"alcohol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f5bde-644b-4a4c-b217-799c83dc90df",
   "metadata": {},
   "source": [
    "### Test Models\n",
    "\n",
    "Using your four selected models, compare their performance on the test set.  \n",
    "Do so using RMSE as your model metric  \n",
    "\n",
    "This part is where it really comes in handy that all four of these models used the same predictors.  This allowed me to setup a simple `for` loop to loop through the four models and get the RMSE of each model by combining `np.sqrt()` and `mean_square_error()` functions.  Of note here is that we are using the test data for this part along with the `.predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b47a8b1c-0b05-4257-a156-bf9715bb0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n",
      "0.8352330798436479\n",
      "Lasso(alpha=0.0008299016722122313)\n",
      "0.8349950933709157\n",
      "Ridge(alpha=3.0)\n",
      "0.8350970502784884\n",
      "ElasticNet(alpha=0.0008468384410328895, l1_ratio=0.98)\n",
      "0.8349911467978637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "models = [mlr_many_fit, lasso_model_fit, ridge_model_fit, net_model_fit]\n",
    "\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    print(np.sqrt(mean_squared_error(y_test[\"alcohol\"], \\\n",
    "       model.predict(X_test[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]]))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b0ee5-6a4a-44d4-8c39-6d3036894f99",
   "metadata": {},
   "source": [
    "All those the RMSE outcomes were close but the Elastic Net model net_model_fit had the lowest RMSE and thus the best performance using the test data by the RMSE criteria.\n",
    "\n",
    "Do so using MAE as your model metric\n",
    "The code for this part only differs from the RMSE code by using the `mean_absolute_error()` function to get MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c97447c2-57a5-4cd3-bc40-7674ef0e740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n",
      "0.7693018277118021\n",
      "Lasso(alpha=0.0008299016722122313)\n",
      "0.7693219349857922\n",
      "Ridge(alpha=3.0)\n",
      "0.7693251680933757\n",
      "ElasticNet(alpha=0.0008468384410328895, l1_ratio=0.98)\n",
      "0.7693226346152401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "models = [mlr_many_fit, lasso_model_fit, ridge_model_fit, net_model_fit]\n",
    "\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    print(np.sqrt(mean_absolute_error(y_test[\"alcohol\"], \\\n",
    "       model.predict(X_test[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a19de-0f49-427d-9bfe-50733a9c5a62",
   "metadata": {},
   "source": [
    "The MLR model `mlr_many_fit` had the lowest MAE and thus the best performance using the test data by the MAE criteria.  Note that this is different outcome than the RMSE outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee894497-8bc1-4199-b669-baa0917aea34",
   "metadata": {},
   "source": [
    "## Classification Task (Wine Type as Response)\n",
    "Repeat the training and testing done previously but use logistic regression models.\n",
    "Use log-loss or negative log-loss as your metric for choosing models during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108b664-5f8b-4a33-b99e-1cc9f21414f2",
   "metadata": {},
   "source": [
    "### Logistic Regression with Multiple Predictors\n",
    "\n",
    "For the Logistic Regression section I'm going to say how this part is similar or different than the earlier Regression Task part.  I will say in general that I'm using the same predictor sets as the corresponding parts earlier.  Of course the response here changes to `type` for all these models.\n",
    "\n",
    "For all these models without regularization, I use `LogisticRegression()` with `penalty = None`.  Most of the models in this part use `solver = \"newton-cholesky\")` except where noted.  Also all models use `cross_validate` with `scoring = neg_log_loss` to do 5 fold CV for later model comparisons.\n",
    "\n",
    "I start with `cv_few_log` which uses the same predictors as the few predictor MLR model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e309e4-e76e-40f2-9965-54d698acd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv_few_log = cross_validate(\n",
    "    LogisticRegression(penalty = None, solver = \"newton-cholesky\"),\n",
    "    X_train[[\"chlorides\", \"pH\"]],\n",
    "    y_train[\"type\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_log_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08fcf9-6e08-47e4-8861-fb97aa5a006b",
   "metadata": {},
   "source": [
    "The second model tested in `cv_many_log` uses the same predictors as the many predictor MLR model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baac5b6f-91c7-4470-8072-754efa8ce41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_many_log = cross_validate(\n",
    "    LogisticRegression(penalty = None, solver = \"newton-cholesky\"),\n",
    "    X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "    y_train[\"type\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_log_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69967b-b60b-4c28-9f6c-d62b27052d5b",
   "metadata": {},
   "source": [
    "The third model tested in `cv_inter_log` uses the same predictors as the MLR basic interaction model using `inter_train` created in the MLR section which added the interaction column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3e7a700-b230-4498-89bd-c09e050f3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_inter_log = cross_validate(\n",
    "    LogisticRegression(penalty = None, solver = \"newton-cholesky\"),\n",
    "    inter_train,\n",
    "    y_train[\"type\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_log_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b189894-2f9b-4671-8c06-25721e4cf9cc",
   "metadata": {},
   "source": [
    "The fourth model tested in `cv_poly_log` uses the same predictors as the MLR model with quadratic and cubic polynomials using `poly_train` created in the MLR section which added the polynomial and interaction columns.  I did find I needed to try a differnt solver `lbfgs` to get this one to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d8e3c7d-9089-4ae6-847e-1fb793952375",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_poly_log = cross_validate(\n",
    "    ##had to switch solver for this one\n",
    "    LogisticRegression(penalty = None, solver = \"lbfgs\", max_iter = 5000),\n",
    "    poly_train,\n",
    "    y_train[\"type\"],\n",
    "    cv = 5,\n",
    "    scoring = \"neg_log_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dabf042-0f3d-4efa-878c-b08a3899e14c",
   "metadata": {},
   "source": [
    "Next I compare the four models based on `neg_log_loss` by getting a mean of the CV generated test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b441c9fb-8b5b-4198-ab14-3ec800c9cd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.313, -0.1395, -0.2823, -0.3992]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(cv_few_log['test_score'].mean(),4), round(cv_many_log['test_score'].mean(),4),\\\n",
    "  round(cv_inter_log['test_score'].mean(),4),  round(cv_poly_log['test_score'].mean(),4) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e24a69-8e42-4218-bc34-83b296ba8bd3",
   "metadata": {},
   "source": [
    "In this case larger is better so best one is second one or `cv_many_log` model.  Next fit that model with the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb964139-f183-4074-b9b6-0632d8bd5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_log = LogisticRegression(penalty = None, solver = \"newton-cholesky\")\\\n",
    ".fit(X = X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "    y = y_train[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6162e0-25ef-4b63-82ba-f89cacd6250d",
   "metadata": {},
   "source": [
    "### Fit a L1 Logistic Regression model with a set of predictors of your choosing\n",
    "As with the MLR code, in this section the next three models the same predictor as the `many_log` model which again simplifies testing later.  All three models use `LogisticRegressionCV()` with 5 fold CV to find the best regularization parameters then use `LogisticRegression()` to fit the models based on those CV found parameters.  They also all use the `saga` solver as I found this works for all of them.  I also used `neg_log_loss` to score these models.  The main differences are the penalty and related parameters.\n",
    "\n",
    "I first need to get the best C parameter for the L1 model.  I only needed to give it the number of `Cs` to use and `l1` to be specific for L1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0401c88a-0d27-42df-a5c9-1add294c2438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53322147])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "cv_lasso_c = LogisticRegressionCV(cv = 5,\n",
    "    solver = \"saga\",\n",
    "    penalty = \"l1\",\n",
    "    Cs = 250,\n",
    "    scoring = \"neg_log_loss\",\n",
    "    random_state = 20)\\\n",
    "   .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"type\"])\n",
    "\n",
    "cv_lasso_c.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e547274-8c51-450f-92b9-f62ecf73b4d1",
   "metadata": {},
   "source": [
    "Next I fit the L1 model and save as `l1_mod_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2af60f54-3105-4b17-8980-61408f35f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_mod_log = LogisticRegression(\n",
    "    solver = \"saga\",\n",
    "    penalty = \"l2\",\n",
    "    C = cv_lasso_c.C_[0],\n",
    "    random_state = 35)\\\n",
    "   .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13248439-ddd9-4c5c-91a9-026b2e704652",
   "metadata": {},
   "source": [
    "### Fit a L2 Logistic Regression model with a set of predictors of your choosing\n",
    "The only difference here from L1 for L2 was using `l2` for penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaa27a0f-8db1-463b-ab66-0cc787ef8a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96368643])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ridge_c = LogisticRegressionCV(cv = 5,\n",
    "    solver = \"saga\",\n",
    "    penalty = \"l2\",\n",
    "    Cs = 250,\n",
    "    scoring = \"neg_log_loss\",\n",
    "    random_state = 20)\\\n",
    "   .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"type\"])\n",
    "\n",
    "cv_ridge_c.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85ab6e-b3f9-4ab6-ae43-453ef0db0626",
   "metadata": {},
   "source": [
    "Next I fit the L2 model and save as `l2_mod_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f43ae103-c320-47b8-a065-308d97a1f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_mod_log = LogisticRegression(\n",
    "    solver = \"saga\",\n",
    "    penalty = \"l2\",\n",
    "    C = cv_ridge_c.C_[0],\n",
    "    random_state = 25)\\\n",
    "   .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0e309-02c0-44cc-b1ca-6af57e5aecd1",
   "metadata": {},
   "source": [
    "### Fit an Elastic Net Logistic Regression model with a set of predictors of your choosing\n",
    "The differences for the Net model from L1 and L2 models are the use of `elasticnet` and the addition of the list of `l1_ratios`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47166998-0124-4599-a7a7-fa07e677d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89496743])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_net_c = LogisticRegressionCV(cv = 5,\n",
    "    solver = \"saga\",\n",
    "    penalty = \"elasticnet\",\n",
    "    l1_ratios = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.98, 0.99, 1],\n",
    "    Cs = 250,\n",
    "    scoring = \"neg_log_loss\",\n",
    "    random_state = 20)\\\n",
    "   .fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"type\"])\n",
    "\n",
    "cv_net_c.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0d685-4254-4432-b241-5cdb72a35339",
   "metadata": {},
   "source": [
    "Next I fit the elastic net penalty model and save as `net_mod_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8acbad3-d8c3-4920-891c-a58aee412889",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_mod_log = LogisticRegression(\n",
    "    solver = \"saga\",\n",
    "    penalty = \"elasticnet\",\n",
    "    C = cv_net_c.C_[0],\n",
    "    l1_ratio = cv_net_c.l1_ratio_[0],\n",
    "    random_state = 25)\\\n",
    ".fit(X_train[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]],\n",
    "         y_train[\"type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb6dd3-7a30-431f-a4d5-23ca823b3b94",
   "metadata": {},
   "source": [
    "### Test Logistic Models\n",
    "\n",
    "During the testing portion, compare your models on both log-loss and accuracy\n",
    "\n",
    "I use a very similar design as in the earlier work with a `for` loop to loop through the logisitic regression models and do the testing.  This is possible because at this point they all use the same predictors.\n",
    "\n",
    "I start the test portion by using `log_loss` for testing along with the `.predict_proba()` method on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f24b0de-6cd8-4bfa-8353-f12f236c2a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(penalty=None, solver='newton-cholesky')\n",
      "0.11435249204599313\n",
      "LogisticRegression(C=0.5332214733067242, random_state=35, solver='saga')\n",
      "0.11552459023680134\n",
      "LogisticRegression(C=0.9636864286572603, random_state=25, solver='saga')\n",
      "0.11499380587719796\n",
      "LogisticRegression(C=0.8949674265472469, l1_ratio=0.1, penalty='elasticnet',\n",
      "                   random_state=25, solver='saga')\n",
      "0.11500203075552815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "models = [many_log, l1_mod_log, l2_mod_log, net_mod_log]\n",
    "\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    proba_pred = \\\n",
    "    model.predict_proba(X_test[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]])\n",
    "    print(log_loss(y_test[\"type\"], proba_pred))\n",
    "                   \n",
    "\n",
    "#minimum is best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ce9cc-9d2f-438c-8e8e-0e54a0fb85ed",
   "metadata": {},
   "source": [
    "In the case of log loss, minimum is better thus the `many_log` logistic regression model without penaly is best by the log loss criteria.\n",
    "\n",
    "The final comparison uses `accuracy_score` which is the only difference in the way this part is coded compared to the log loss part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0de8592-aa5d-4b6d-8a11-3ddd8bf83958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(penalty=None, solver='newton-cholesky')\n",
      "0.9584615384615385\n",
      "LogisticRegression(C=0.5332214733067242, random_state=35, solver='saga')\n",
      "0.9569230769230769\n",
      "LogisticRegression(C=0.9636864286572603, random_state=25, solver='saga')\n",
      "0.9569230769230769\n",
      "LogisticRegression(C=0.8949674265472469, l1_ratio=0.1, penalty='elasticnet',\n",
      "                   random_state=25, solver='saga')\n",
      "0.9569230769230769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import  accuracy_score\n",
    "\n",
    "for model in models:\n",
    "    print(str(model))\n",
    "    preds = \\\n",
    "    model.predict(X_test[[\"chlorides\", \"pH\", \"citric acid\", \"density\", \"sulphates\", \"fixed acidity\" ]])\n",
    "    print(accuracy_score(y_test[\"type\"], preds))\n",
    "                   \n",
    "        \n",
    " #highest is best here       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb0881-aaec-4e6f-a6ea-4b27ac802378",
   "metadata": {},
   "source": [
    "In the accuracy case higher scores are better thus the `many_log` logistic regression without penalty was the best model by accuracy criteria.  In the logistic regression case both criteria led the same model selected based on test data performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
